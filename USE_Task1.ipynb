{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UniversalType2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CobVWhbx8wb8",
        "RVok0xVR8wcW",
        "jqLDeo1g8wcm"
      ],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD5mU-sQ8wbW",
        "colab_type": "text"
      },
      "source": [
        "# Install Tensorflow Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ariTxXxy8wbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eaoxi3q3jLYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "np.random.seed(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up5cVu30jUcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKDI0cT3EvOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqFxU_AA8wb3",
        "colab_type": "text"
      },
      "source": [
        "# Build train, validation and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeVGlJsTQte8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/Dragonet95/utils/raw/master/testall.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scgx6MSbZbtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/Dragonet95/utils/raw/master/toevaluate2.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-J979QYmlHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/Dragonet95/utils/raw/master/politifactfull.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehTOFqwcR0Cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_COL_NAMES = ['line_number', 'speaker', 'text', 'label']\n",
        "dataset = pd.read_csv('testall.csv', index_col=None, header=None, names=_COL_NAMES, sep='\\t')\n",
        "\n",
        "dataset['label'] = [1 if sentiment == '1' else 0 for sentiment in dataset['label'].values]\n",
        "\n",
        "dataset = dataset[1:]\n",
        "dataset.info()\n",
        "reviews = dataset['text'].values\n",
        "sentiments = dataset['label'].values\n",
        "\n",
        "testing = dataset[['label','text']]\n",
        "df_train = testing[:16418]\n",
        "#df_test = testing[15000:]\n",
        "df_train.info()\n",
        "\n",
        "speaker = dataset[['speaker']]\n",
        "df_train2 = speaker[:16418]\n",
        "df_train2.info()\n",
        "\n",
        "_COL_NAMES = ['line_number', 'speaker', 'text', 'label']\n",
        "dataset = pd.read_csv('toevaluate2.csv', index_col=None, header=None, names=_COL_NAMES, sep='\\t')\n",
        "dataset.info()\n",
        "\n",
        "dataset = dataset[1:]\n",
        "\n",
        "reviews = dataset['text'].values\n",
        "\n",
        "testing = dataset[['label','text']]\n",
        "\n",
        "speaker = dataset[['speaker']]\n",
        "df_test2 = speaker[:16418]\n",
        "\n",
        "df_test = testing\n",
        "df_test.info()\n",
        "df_test2.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLXixaMzlLdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_COL_NAMES = ['id','statement', 'label']\n",
        "dataset = pd.read_csv('politifactfull.csv',  index_col=None,names=_COL_NAMES, sep='\\t',skiprows=1)\n",
        "dataset.info()\n",
        "\n",
        "dataset.label[dataset.label=='true'] = '5'\n",
        "dataset.label[dataset.label=='false'] = '0'\n",
        "dataset.label[dataset.label=='barely-true'] = '3'\n",
        "dataset.label[dataset.label=='mostly-true'] = '4'\n",
        "dataset.label[dataset.label=='half-true'] = '2'\n",
        "dataset.label[dataset.label=='pants-fire'] = '1'\n",
        "\n",
        "dataset['label'] = [0 if sentiment == '0' else 1 if sentiment == '1' else 2 if sentiment == '2' else 3 if sentiment == '3' else 4 if sentiment == '4' else 5 if sentiment == '5' else 0 for sentiment in dataset['label'].values]\n",
        "\n",
        "\n",
        "dataset.info()\n",
        "dataset=dataset[['statement','label']]\n",
        "print(dataset)\n",
        "\n",
        "df_train = dataset[:10000]\n",
        "df_test = dataset[10000:len(dataset)]\n",
        "\n",
        "df_train.info()\n",
        "df_test.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYcOLaogmZhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing = dataset[['speaker']]\n",
        "df_train2 = testing[:15000]\n",
        "df_test2 = testing[15000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CobVWhbx8wb8",
        "colab_type": "text"
      },
      "source": [
        "# Basic Text Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0r-ijKZ8wb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install contractions\n",
        "!pip install beautifulsoup4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWGjd8JC8wcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def pre_process_document(document):\n",
        "    \n",
        "    # strip HTML\n",
        "    document = strip_html_tags(document)\n",
        "    \n",
        "    # lower case\n",
        "    document = document.lower()\n",
        "    \n",
        "    # remove extra newlines (often might be present in really noisy text)\n",
        "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    \n",
        "    # remove accented characters\n",
        "    document = remove_accented_chars(document)\n",
        "    \n",
        "    # expand contractions    \n",
        "    document = expand_contractions(document)\n",
        "               \n",
        "    # remove special characters and\\or digits    \n",
        "    # insert spaces between special characters to isolate them    \n",
        "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
        "    document = remove_special_characters(document, remove_digits=True)  \n",
        "        \n",
        "    # remove extra whitespace\n",
        "    document = re.sub(' +', ' ', document)\n",
        "    document = document.strip()\n",
        "    \n",
        "    return document\n",
        "\n",
        "\n",
        "pre_process_corpus = np.vectorize(pre_process_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRdyk3B48wcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_reviews = pre_process_corpus(train_reviews)\n",
        "val_reviews = pre_process_corpus(val_reviews)\n",
        "test_reviews = pre_process_corpus(test_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrChlIE88wcG",
        "colab_type": "text"
      },
      "source": [
        "# Build Data Ingestion Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyeTN9RklIto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emT4HT72lZ2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "category_counts = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvTuNiqYlKCK",
        "colab_type": "code",
        "outputId": "78dd36eb-bb7f-4a53-e1f4-8fa03cd9f33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "from keras import Sequential, Model, Input\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, GlobalAveragePooling1D, Dropout, LSTM, CuDNNLSTM, RNN, SimpleRNN, Conv2D, GlobalMaxPooling1D\n",
        "from keras import callbacks\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "#input_text2 = layers.Input(shape=(1,), dtype=tf.string)\n",
        "#input_text3 = layers.Input(shape=(1,), dtype=tf.string)\n",
        "#embedding1 = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)\n",
        "#embedding2 = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text2)\n",
        "#embedding3 = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text3)\n",
        "\n",
        "#embedding = concatenate([x.output, y.output])\n",
        "#embedding = layers.Add()([embedding1,embedding2])\n",
        "#embedding = layers.Add()([embedding1,embedding2,embedding3])\n",
        "embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)\n",
        "\n",
        "#conv = Conv1D(32, kernel_size=3, activation='elu', padding='same')(embedding)\n",
        "dense = layers.Dense(256, activation='relu')(embedding)\n",
        "\n",
        "pred = layers.Dense(category_counts, activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=input_text, outputs=pred)\n",
        "#model = Model(inputs=[input_text,input_text2], outputs=pred)\n",
        "#model = Model(inputs=[input_text,input_text2, input_text3], outputs=pred)\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0611 10:01:26.019312 140399681935232 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 1542      \n",
            "=================================================================\n",
            "Total params: 132,870\n",
            "Trainable params: 132,870\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWHUIM1olgpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = df_train['statement'].tolist()\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "\n",
        "train_label = np.asarray(pd.get_dummies(df_train.label), dtype = np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zloVPuJnAeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text2 = df_train2['speaker'].tolist()\n",
        "train_text2 = np.array(train_text2, dtype=object)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BBSU86-xz2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text2 = df_train['text'].tolist()\n",
        "train_text2 = [\"\"] + train_text2[:-1]\n",
        "train_text2 = np.array(train_text2, dtype=object)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnAlg2-xEda0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text3 = df_train['text'].tolist()\n",
        "train_text3 = [\"\"] + train_text3[:-1]\n",
        "train_text3 = [\"\"] + train_text3[:-1]\n",
        "\n",
        "train_text3 = np.array(train_text3, dtype=object)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5qijeDcGiV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text3 = df_train['text'].tolist()\n",
        "train_text3 = train_text3[1:] + [\"\"]\n",
        "\n",
        "train_text3 = np.array(train_text3, dtype=object)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivxG3t2FmXoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text = df_test['statement'].tolist()\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "test_label = np.asarray(pd.get_dummies(df_test.label), dtype = np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UY35aUYnMeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text2 = df_test2['speaker'].tolist()\n",
        "test_text2 = np.array(test_text2, dtype=object)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGarfTnl02ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text2 = df_test['text'].tolist()\n",
        "test_text2 = [\"\"] + test_text2[:-1]\n",
        "test_text2 = np.array(test_text2, dtype=object)[:, np.newaxis]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj1wYsA7ErIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text3 = df_test['text'].tolist()\n",
        "test_text3 = [\"\"] + test_text3[:-1]\n",
        "test_text3 = [\"\"] + test_text3[:-1]\n",
        "test_text3 = np.array(test_text3, dtype=object)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI77goIJKgQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text2 = df_test['text'].tolist()\n",
        "test_text2 = test_text2[1:]+ [\"\"]\n",
        "test_text2 = np.array(test_text2, dtype=object)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOVEs6XKmb63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  history = model.fit( [train_text, train_text2],\n",
        "            train_label,\n",
        "            epochs=5,\n",
        "            batch_size=32)\n",
        "  model.save_weights('./model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arzV_LhU2S0B",
        "colab_type": "code",
        "outputId": "e954967c-5383-4a20-ed46-47fb52fb11cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  history = model.fit( train_text,\n",
        "            train_label,\n",
        "            validation_data=(test_text,test_label),\n",
        "            epochs=10,\n",
        "            batch_size=32)\n",
        "  model.save_weights('./model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 2791 samples\n",
            "Epoch 1/10\n",
            "  416/10000 [>.............................] - ETA: 18s - loss: 0.1381 - acc: 0.2067"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fb113669a20>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
            "    self._session._session, self._handle, status)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
            "    c_api.TF_GetCode(self.status.status))\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 9888/10000 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.2276"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fb1175ad2b0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
            "    self._session._session, self._handle, status)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
            "    c_api.TF_GetCode(self.status.status))\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 6s 626us/step - loss: 0.1360 - acc: 0.2273 - val_loss: 0.1348 - val_acc: 0.2386\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 5s 513us/step - loss: 0.1341 - acc: 0.2621 - val_loss: 0.1341 - val_acc: 0.2569\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 5s 514us/step - loss: 0.1330 - acc: 0.2762 - val_loss: 0.1341 - val_acc: 0.2644\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 5s 510us/step - loss: 0.1318 - acc: 0.2933 - val_loss: 0.1338 - val_acc: 0.2569\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 5s 515us/step - loss: 0.1304 - acc: 0.3140 - val_loss: 0.1338 - val_acc: 0.2616\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 5s 490us/step - loss: 0.1289 - acc: 0.3377 - val_loss: 0.1338 - val_acc: 0.2641\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 5s 501us/step - loss: 0.1271 - acc: 0.3592 - val_loss: 0.1346 - val_acc: 0.2608\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 5s 494us/step - loss: 0.1251 - acc: 0.3752 - val_loss: 0.1348 - val_acc: 0.2598\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 5s 503us/step - loss: 0.1229 - acc: 0.4009 - val_loss: 0.1353 - val_acc: 0.2644\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 5s 503us/step - loss: 0.1207 - acc: 0.4216 - val_loss: 0.1356 - val_acc: 0.2565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yvKwI8dmiNV",
        "colab_type": "code",
        "outputId": "63b54e5f-c4c8-4ebf-8c5d-a09c0823868f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -alh | grep model.h5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 530K May  6 13:30 model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVok0xVR8wcW",
        "colab_type": "text"
      },
      "source": [
        "# Build Deep Learning Model with Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20g5jb-K8wcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_feature = hub.text_embedding_column(\n",
        "    key='sentence', \n",
        "    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\",\n",
        "    trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh1pXesx8wcc",
        "colab_type": "code",
        "outputId": "f14c6f5b-05aa-4338-bace-98fa31cf618f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "dnn = tf.estimator.DNNClassifier(\n",
        "          hidden_units=[512, 128],\n",
        "          feature_columns=[embedding_feature],\n",
        "          n_classes=2,\n",
        "          activation_fn=tf.nn.relu,\n",
        "          dropout=0.1,\n",
        "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0419 18:19:28.325523 140148631795584 estimator.py:1739] Using default config.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp6fafmxrs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0419 18:19:28.335888 140148631795584 estimator.py:1760] Using temporary folder as model directory: /tmp/tmp6fafmxrs\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp6fafmxrs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f767e3c6c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0419 18:19:28.338926 140148631795584 estimator.py:201] Using config: {'_model_dir': '/tmp/tmp6fafmxrs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f767e3c6c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnTwC2VN8wch",
        "colab_type": "text"
      },
      "source": [
        "### Train for approx 12 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqLDeo1g8wcm",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z14Um_xQ8wcn",
        "colab_type": "code",
        "outputId": "b9dd127b-add2-4d52-8ff8-b31d3108d5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1669
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "import time\n",
        "\n",
        "TOTAL_STEPS = 1500\n",
        "STEP_SIZE = 100\n",
        "for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
        "    print()\n",
        "    print('-'*100)\n",
        "    print('Training for step =', step)\n",
        "    start_time = time.time()\n",
        "    dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print('Train Time (s):', elapsed_time)\n",
        "    print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n",
        "    print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 0\n",
            "Train Time (s): 50.60619783401489\n",
            "Eval Metrics (Train): {'accuracy': 0.5685652, 'accuracy_baseline': 0.5502174, 'auc': 0.9439395, 'auc_precision_recall': 0.93231857, 'average_loss': 0.92657685, 'label/mean': 0.4497826, 'loss': 118.39593, 'precision': 1.0, 'prediction/mean': 0.09654394, 'recall': 0.040792655, 'global_step': 100}\n",
            "Eval Metrics (Validation): {'accuracy': 0.956, 'accuracy_baseline': 0.956, 'auc': 0.67050207, 'auc_precision_recall': 0.07705936, 'average_loss': 0.19451128, 'label/mean': 0.044, 'loss': 24.31391, 'precision': 0.0, 'prediction/mean': 0.021401588, 'recall': 0.0, 'global_step': 100}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 100\n",
            "Train Time (s): 46.15957498550415\n",
            "Eval Metrics (Train): {'accuracy': 0.6458261, 'accuracy_baseline': 0.5502174, 'auc': 0.94975805, 'auc_precision_recall': 0.93943965, 'average_loss': 0.74575025, 'label/mean': 0.4497826, 'loss': 95.29031, 'precision': 0.9937135, 'prediction/mean': 0.14758664, 'recall': 0.21391977, 'global_step': 200}\n",
            "Eval Metrics (Validation): {'accuracy': 0.955, 'accuracy_baseline': 0.956, 'auc': 0.7074221, 'auc_precision_recall': 0.0884697, 'average_loss': 0.18901278, 'label/mean': 0.044, 'loss': 23.626598, 'precision': 0.0, 'prediction/mean': 0.027694335, 'recall': 0.0, 'global_step': 200}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 200\n",
            "Train Time (s): 47.36938500404358\n",
            "Eval Metrics (Train): {'accuracy': 0.732, 'accuracy_baseline': 0.5502174, 'auc': 0.95501924, 'auc_precision_recall': 0.9456706, 'average_loss': 0.6947156, 'label/mean': 0.4497826, 'loss': 88.76922, 'precision': 0.6278515, 'prediction/mean': 0.697865, 'recall': 0.99236345, 'global_step': 300}\n",
            "Eval Metrics (Validation): {'accuracy': 0.586, 'accuracy_baseline': 0.956, 'auc': 0.72489536, 'auc_precision_recall': 0.11310412, 'average_loss': 1.0845752, 'label/mean': 0.044, 'loss': 135.5719, 'precision': 0.07954545, 'prediction/mean': 0.4385805, 'recall': 0.79545456, 'global_step': 300}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 300\n",
            "Train Time (s): 46.142093658447266\n",
            "Eval Metrics (Train): {'accuracy': 0.74726087, 'accuracy_baseline': 0.5502174, 'auc': 0.95980585, 'auc_precision_recall': 0.9522434, 'average_loss': 0.53822166, 'label/mean': 0.4497826, 'loss': 68.772766, 'precision': 0.9864749, 'prediction/mean': 0.22113054, 'recall': 0.44417593, 'global_step': 400}\n",
            "Eval Metrics (Validation): {'accuracy': 0.952, 'accuracy_baseline': 0.956, 'auc': 0.72011703, 'auc_precision_recall': 0.09875104, 'average_loss': 0.1919013, 'label/mean': 0.044, 'loss': 23.987661, 'precision': 0.25, 'prediction/mean': 0.036305875, 'recall': 0.045454547, 'global_step': 400}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 400\n",
            "Train Time (s): 46.203763484954834\n",
            "Eval Metrics (Train): {'accuracy': 0.71347827, 'accuracy_baseline': 0.5502174, 'auc': 0.96022713, 'auc_precision_recall': 0.95288867, 'average_loss': 0.6281127, 'label/mean': 0.4497826, 'loss': 80.258835, 'precision': 0.9908497, 'prediction/mean': 0.18962112, 'recall': 0.36636057, 'global_step': 500}\n",
            "Eval Metrics (Validation): {'accuracy': 0.957, 'accuracy_baseline': 0.956, 'auc': 0.70620966, 'auc_precision_recall': 0.09980491, 'average_loss': 0.19549806, 'label/mean': 0.044, 'loss': 24.437258, 'precision': 0.6666667, 'prediction/mean': 0.027254218, 'recall': 0.045454547, 'global_step': 500}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 500\n",
            "Train Time (s): 46.15818691253662\n",
            "Eval Metrics (Train): {'accuracy': 0.8996522, 'accuracy_baseline': 0.5502174, 'auc': 0.9656339, 'auc_precision_recall': 0.95938706, 'average_loss': 0.23995963, 'label/mean': 0.4497826, 'loss': 30.661507, 'precision': 0.88035965, 'prediction/mean': 0.45763057, 'recall': 0.8990817, 'global_step': 600}\n",
            "Eval Metrics (Validation): {'accuracy': 0.883, 'accuracy_baseline': 0.956, 'auc': 0.72317183, 'auc_precision_recall': 0.10525247, 'average_loss': 0.29351562, 'label/mean': 0.044, 'loss': 36.689453, 'precision': 0.10752688, 'prediction/mean': 0.14174084, 'recall': 0.22727273, 'global_step': 600}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 600\n",
            "Train Time (s): 46.50796961784363\n",
            "Eval Metrics (Train): {'accuracy': 0.8795217, 'accuracy_baseline': 0.5502174, 'auc': 0.965857, 'auc_precision_recall': 0.95936686, 'average_loss': 0.2784631, 'label/mean': 0.4497826, 'loss': 35.581394, 'precision': 0.94313127, 'prediction/mean': 0.3693902, 'recall': 0.7791203, 'global_step': 700}\n",
            "Eval Metrics (Validation): {'accuracy': 0.919, 'accuracy_baseline': 0.956, 'auc': 0.72921026, 'auc_precision_recall': 0.1068791, 'average_loss': 0.22089925, 'label/mean': 0.044, 'loss': 27.612408, 'precision': 0.10638298, 'prediction/mean': 0.082656935, 'recall': 0.11363637, 'global_step': 700}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 700\n",
            "Train Time (s): 53.01059579849243\n",
            "Eval Metrics (Train): {'accuracy': 0.89, 'accuracy_baseline': 0.5502174, 'auc': 0.9665501, 'auc_precision_recall': 0.9601426, 'average_loss': 0.25695547, 'label/mean': 0.4497826, 'loss': 32.8332, 'precision': 0.93057853, 'prediction/mean': 0.39249986, 'recall': 0.8163364, 'global_step': 800}\n",
            "Eval Metrics (Validation): {'accuracy': 0.913, 'accuracy_baseline': 0.956, 'auc': 0.73411953, 'auc_precision_recall': 0.117359385, 'average_loss': 0.23337446, 'label/mean': 0.044, 'loss': 29.171808, 'precision': 0.094339624, 'prediction/mean': 0.092938006, 'recall': 0.11363637, 'global_step': 800}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 800\n",
            "Train Time (s): 48.48436951637268\n",
            "Eval Metrics (Train): {'accuracy': 0.7514348, 'accuracy_baseline': 0.5502174, 'auc': 0.96485263, 'auc_precision_recall': 0.9586073, 'average_loss': 0.5425675, 'label/mean': 0.4497826, 'loss': 69.32807, 'precision': 0.98963183, 'prediction/mean': 0.22086908, 'recall': 0.45210245, 'global_step': 900}\n",
            "Eval Metrics (Validation): {'accuracy': 0.954, 'accuracy_baseline': 0.956, 'auc': 0.70685154, 'auc_precision_recall': 0.114815935, 'average_loss': 0.19499335, 'label/mean': 0.044, 'loss': 24.374168, 'precision': 0.33333334, 'prediction/mean': 0.030345175, 'recall': 0.045454547, 'global_step': 900}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 900\n",
            "Train Time (s): 49.63564085960388\n",
            "Eval Metrics (Train): {'accuracy': 0.75652176, 'accuracy_baseline': 0.5502174, 'auc': 0.9655408, 'auc_precision_recall': 0.9596713, 'average_loss': 0.5267425, 'label/mean': 0.4497826, 'loss': 67.30599, 'precision': 0.9896801, 'prediction/mean': 0.2256938, 'recall': 0.46350893, 'global_step': 1000}\n",
            "Eval Metrics (Validation): {'accuracy': 0.954, 'accuracy_baseline': 0.956, 'auc': 0.7052349, 'auc_precision_recall': 0.11499694, 'average_loss': 0.19342193, 'label/mean': 0.044, 'loss': 24.177742, 'precision': 0.33333334, 'prediction/mean': 0.031564094, 'recall': 0.045454547, 'global_step': 1000}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 1000\n",
            "Train Time (s): 48.81299877166748\n",
            "Eval Metrics (Train): {'accuracy': 0.90334785, 'accuracy_baseline': 0.5502174, 'auc': 0.9690545, 'auc_precision_recall': 0.9633235, 'average_loss': 0.23267213, 'label/mean': 0.4497826, 'loss': 29.730328, 'precision': 0.86724544, 'prediction/mean': 0.48213246, 'recall': 0.92701787, 'global_step': 1100}\n",
            "Eval Metrics (Validation): {'accuracy': 0.874, 'accuracy_baseline': 0.956, 'auc': 0.73152816, 'auc_precision_recall': 0.11924893, 'average_loss': 0.32197094, 'label/mean': 0.044, 'loss': 40.24637, 'precision': 0.09803922, 'prediction/mean': 0.15240924, 'recall': 0.22727273, 'global_step': 1100}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 1100\n",
            "Train Time (s): 47.90294694900513\n",
            "Eval Metrics (Train): {'accuracy': 0.8094348, 'accuracy_baseline': 0.5502174, 'auc': 0.9657062, 'auc_precision_recall': 0.95850396, 'average_loss': 0.48418796, 'label/mean': 0.4497826, 'loss': 61.86846, 'precision': 0.7059271, 'prediction/mean': 0.6289149, 'recall': 0.9878202, 'global_step': 1200}\n",
            "Eval Metrics (Validation): {'accuracy': 0.686, 'accuracy_baseline': 0.956, 'auc': 0.72607213, 'auc_precision_recall': 0.12513188, 'average_loss': 0.76795423, 'label/mean': 0.044, 'loss': 95.99428, 'precision': 0.08074534, 'prediction/mean': 0.32886052, 'recall': 0.59090906, 'global_step': 1200}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 1200\n",
            "Train Time (s): 47.218749046325684\n",
            "Eval Metrics (Train): {'accuracy': 0.79234785, 'accuracy_baseline': 0.5502174, 'auc': 0.9698885, 'auc_precision_recall': 0.9648886, 'average_loss': 0.4534937, 'label/mean': 0.4497826, 'loss': 57.94642, 'precision': 0.9863756, 'prediction/mean': 0.2562642, 'recall': 0.54586756, 'global_step': 1300}\n",
            "Eval Metrics (Validation): {'accuracy': 0.951, 'accuracy_baseline': 0.956, 'auc': 0.69422776, 'auc_precision_recall': 0.117166996, 'average_loss': 0.19966407, 'label/mean': 0.044, 'loss': 24.95801, 'precision': 0.27272728, 'prediction/mean': 0.03395028, 'recall': 0.06818182, 'global_step': 1300}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 1300\n",
            "Train Time (s): 48.63186073303223\n",
            "Eval Metrics (Train): {'accuracy': 0.8475217, 'accuracy_baseline': 0.5502174, 'auc': 0.96953183, 'auc_precision_recall': 0.9635641, 'average_loss': 0.367752, 'label/mean': 0.4497826, 'loss': 46.990532, 'precision': 0.75473106, 'prediction/mean': 0.58591783, 'recall': 0.979217, 'global_step': 1400}\n",
            "Eval Metrics (Validation): {'accuracy': 0.757, 'accuracy_baseline': 0.956, 'auc': 0.7215671, 'auc_precision_recall': 0.1215221, 'average_loss': 0.5950609, 'label/mean': 0.044, 'loss': 74.382614, 'precision': 0.08713693, 'prediction/mean': 0.26730075, 'recall': 0.47727272, 'global_step': 1400}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 1400\n",
            "Train Time (s): 47.20482516288757\n",
            "Eval Metrics (Train): {'accuracy': 0.87865216, 'accuracy_baseline': 0.5502174, 'auc': 0.9726479, 'auc_precision_recall': 0.96790403, 'average_loss': 0.27708268, 'label/mean': 0.4497826, 'loss': 35.405006, 'precision': 0.96366316, 'prediction/mean': 0.35266933, 'recall': 0.7588207, 'global_step': 1500}\n",
            "Eval Metrics (Validation): {'accuracy': 0.928, 'accuracy_baseline': 0.956, 'auc': 0.7259296, 'auc_precision_recall': 0.119415216, 'average_loss': 0.2134976, 'label/mean': 0.044, 'loss': 26.687199, 'precision': 0.0882353, 'prediction/mean': 0.06292363, 'recall': 0.06818182, 'global_step': 1500}\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Training for step = 1500\n",
            "Train Time (s): 50.14292502403259\n",
            "Eval Metrics (Train): {'accuracy': 0.80104345, 'accuracy_baseline': 0.5502174, 'auc': 0.9719556, 'auc_precision_recall': 0.9674443, 'average_loss': 0.435832, 'label/mean': 0.4497826, 'loss': 55.689644, 'precision': 0.9871643, 'prediction/mean': 0.26266986, 'recall': 0.56500727, 'global_step': 1600}\n",
            "Eval Metrics (Validation): {'accuracy': 0.948, 'accuracy_baseline': 0.956, 'auc': 0.6921714, 'auc_precision_recall': 0.1181552, 'average_loss': 0.20101926, 'label/mean': 0.044, 'loss': 25.127407, 'precision': 0.21428572, 'prediction/mean': 0.034247648, 'recall': 0.06818182, 'global_step': 1600}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OB2i48m8wcr",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEp-LrZanK1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights('./model.h5')  \n",
        "  predicts = model.predict([test_text,test_text2], batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN44i8Ri2t1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights('./model.h5')  \n",
        "  predicts = model.predict(test_text, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L63Roswpldk",
        "colab_type": "code",
        "outputId": "a297b44a-171d-4a80-dd15-357462aa42b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights('./model.h5')  \n",
        "  a,b = model.evaluate(test_text,test_label, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 576/2791 [=====>........................] - ETA: 2s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fb1166e18d0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
            "    self._session._session, self._handle, status)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
            "    c_api.TF_GetCode(self.status.status))\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2791/2791 [==============================] - 1s 536us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60zJval1VOM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "folder = \"mse3\"\n",
        "np.set_printoptions(suppress=True)\n",
        "os.mkdir(folder)\n",
        "\n",
        "predictions = []\n",
        "cont=0\n",
        "for x in predicts:\n",
        "  x = str(x)\n",
        "  start = x.find(' ') + 1\n",
        "  end = x.find(']', start)\n",
        "  predictions.append( float(x[start:end]))\n",
        "  #print(float(x[start:end]))\n",
        " \n",
        "with open(folder+'/primary-20151219_3_dem.tsv', 'w') as f:\n",
        "  writer=csv.writer(f, delimiter='\\t')\n",
        "  for i,j in enumerate(predictions[:1388],start=1):\n",
        "    print(\"{}\\t{:.15f}\".format(i, j))\n",
        "    writer.writerow([i,\"{:.15f}\".format(j)])\n",
        "    \n",
        "with open(folder+'/primary-20160129_7_gop.tsv', 'w') as f:\n",
        "  writer=csv.writer(f, delimiter='\\t')\n",
        "  for i,j in enumerate(predictions[1388:2868],start=1):\n",
        "    print(\"{}\\t{:.15f}\".format(i, j))\n",
        "    writer.writerow([i,\"{:.15f}\".format(j)])\n",
        "    \n",
        "with open(folder+'/primary-20160311_12_gop.tsv', 'w') as f:\n",
        "  writer=csv.writer(f, delimiter='\\t')\n",
        "  for i,j in enumerate(predictions[2868:4586],start=1):\n",
        "    print(\"{}\\t{:.15f}\".format(i, j))\n",
        "    writer.writerow([i,\"{:.15f}\".format(j)])\n",
        "    \n",
        "with open(folder+'/primary-20180131_state_union.tsv', 'w') as f:\n",
        "  writer=csv.writer(f, delimiter='\\t')\n",
        "  for i,j in enumerate(predictions[4586:5106],start=1):\n",
        "    print(\"{}\\t{:.15f}\".format(i, j))\n",
        "    writer.writerow([i,\"{:.15f}\".format(j)])\n",
        "    \n",
        "with open(folder+'/primary-20181015_60_min.tsv', 'w') as f:\n",
        "  writer=csv.writer(f, delimiter='\\t')\n",
        "  for i,j in enumerate(predictions[5106:5718],start=1):\n",
        "    print(\"{}\\t{:.15f}\".format(i, j))\n",
        "    writer.writerow([i,\"{:.15f}\".format(j)])\n",
        "    \n",
        "with open(folder+'/primary-20190205_trump_state.tsv', 'w') as f:\n",
        "  writer=csv.writer(f, delimiter='\\t')\n",
        "  for i,j in enumerate(predictions[5718:6222],start=1):\n",
        "    print(\"{}\\t{:.15f}\".format(i, j))\n",
        "    writer.writerow([i,\"{:.15f}\".format(j)])\n",
        "    \n",
        "with open(folder+'/primary-20190215_trump_emergency.tsv', 'w') as f:\n",
        "  writer=csv.writer(f, delimiter='\\t')\n",
        "  for i,j in enumerate(predictions[6222:7080],start=1):\n",
        "    print(\"{}\\t{:.15f}\".format(i, j))\n",
        "    writer.writerow([i,\"{:.15f}\".format(j)])\n",
        "    \n",
        "!zip -r mse3.zip mse3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dgn0M_sWRuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newlist = []\n",
        "cont=0\n",
        "for x in predicts[-316:-71]:\n",
        "  x = str(x)\n",
        "  start = x.find(' ') + 1\n",
        "  end = x.find(']', start)\n",
        "  newlist.append( float(x[start:end]))\n",
        "  #print(float(x[start:end]))\n",
        "\n",
        "for x in newlist:\n",
        "  cont = cont+1\n",
        "  print('%s\\t%.15f' %(cont, x) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx2MPfU-WYOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newlist = []\n",
        "cont=0\n",
        "for x in predicts[-762:-316]:\n",
        "  x = str(x)\n",
        "  start = x.find(' ') + 1\n",
        "  end = x.find(']', start)\n",
        "  newlist.append( float(x[start:end]))\n",
        "  #print(float(x[start:end]))\n",
        "\n",
        "for x in newlist:\n",
        "  cont = cont+1\n",
        "  print('%s\\t%.15f' %(cont, x) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4H5JjsU7mva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(estimator, input_fn):\n",
        "    return [x[\"probabilities\"][0] for x in estimator.predict(input_fn=input_fn)]\n",
        "predictions = get_predictions(estimator=dnn, input_fn=predict_test_input_fn)\n",
        "for i,j in enumerate(predictions[-316:-71],start=1):\n",
        "  print(\"{}\\t{:.15f}\".format(i, 1-j))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL2Ky-Tvda6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(estimator, input_fn):\n",
        "    return [x[\"probabilities\"][0] for x in estimator.predict(input_fn=input_fn)]\n",
        "predictions = get_predictions(estimator=dnn, input_fn=predict_test_input_fn)\n",
        "for i,j in enumerate(predictions[-762:-316],start=1):\n",
        "  print(\"{}\\t{:.15f}\".format(i, 1-j))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
